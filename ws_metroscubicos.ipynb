{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program to scrape www.metroscubicos.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import json\n",
    "url = \"https://www.inmuebles24.com/departamentos-en-venta-en-del-carmen.html\"\n",
    "root_url = \"https://www.inmuebles24.com\"\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import json\n",
    "\n",
    "class WebScraping(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        json_file_name = 'ws_parameters.json'\n",
    "        source_to_test = 'metroscubicos'\n",
    "\n",
    "        # open json file to get page ID and token\n",
    "        with open(json_file_name, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.main_url = data['app']['sources'][source_to_test]['main_url']\n",
    "        self.output_file_name = data['app']['sources'][source_to_test]['output_file_name']\n",
    "        self.debug = data['app']['sources'][source_to_test]['debug']\n",
    "        self.delimiter = data['app']['sources'][source_to_test]['delimiter']\n",
    "        \n",
    "        if self.debug == 'Y':\n",
    "            print('In WebScraping')\n",
    "            print(self.main_url)\n",
    "            \n",
    "    def write_file(self, p_line):\n",
    "        self.output_file.write(p_line + \"\\n\")\n",
    "\n",
    "    def next_page(self, p_soup):\n",
    "        new_url = 'None'\n",
    "        pag_container = p_soup.find_all(\"a\", class_=\"andes-pagination__link prefetch\")\n",
    "        if self.debug == 'Y':\n",
    "            print('Type: ' + str(type(pag_container)) + ' with ' + str(len(pag_container)) + ' elements')\n",
    "        for item in pag_container:\n",
    "            new_url = item.get('href')\n",
    "        if self.debug == 'Y':\n",
    "            print(new_url)\n",
    "        return new_url\n",
    "    \n",
    "    def get_url(self, p_data_links, p_id, p_initial_id):\n",
    "# There is no need to loop the entire dataset, p_initial_id parameter helped to solve this\n",
    "#        item_url = ''\n",
    "#        for j, itemj in enumerate(p_data_links):\n",
    "#            if j + len(p_data_links) == p_id:\n",
    "#                item_url = itemj['href']\n",
    "#                break\n",
    "        item_url = p_data_links[p_id-p_initial_id]['href']\n",
    "        return item_url\n",
    "\n",
    "    def read_website(self, p_url):\n",
    "        if self.debug == 'Y':\n",
    "            print('In read_website')\n",
    "        \n",
    "        url_to_search = p_url\n",
    "        k = 0 # To manage the counter accross pages\n",
    "        \n",
    "        while url_to_search != 'None':\n",
    "            print('Searching ' + url_to_search)\n",
    "            page = requests.get(url_to_search)\n",
    "            soup = bs4.BeautifulSoup(page.content, 'html.parser')\n",
    "            data_block = soup.find_all(\"div\", class_=\"item__info\")\n",
    "            data_links = soup.find_all(\"a\", class_=\"item__info-link\")\n",
    "            if self.debug == 'Y':\n",
    "                print('data_block type: ' + str(type(data_block)) + ' with ' + str(len(data_block)) + ' elements')\n",
    "                print('data_links type: ' + str(type(data_links)) + ' with ' + str(len(data_links)) + ' elements')\n",
    "\n",
    "            for i, item in enumerate(data_block):\n",
    "                prices = item.find_all(\"span\", class_=\"price__symbol\")\n",
    "                pricev = item.find_all(\"span\", class_=\"price__fraction\")\n",
    "                title = item.find_all(\"div\", class_=\"item__title\")\n",
    "                htype = item.find_all(\"p\", class_=\"item__info-title\")\n",
    "                descr = item.find_all(\"div\", class_=\"item__attrs\")\n",
    "                item_url = self.get_url(data_links, i+k, k)\n",
    "                line = str(i+k+1) + self.delimiter + prices[0].contents[0] + self.delimiter + pricev[0].contents[0] + self.delimiter + title[0].contents[0] + self.delimiter + htype[0].contents[0] + self.delimiter + descr[0].contents[0] + self.delimiter + item_url\n",
    "                self.write_file(line)\n",
    "\n",
    "                if self.debug == 'Y':\n",
    "                    print(f'{str(i+k+1)} {self.delimiter} {prices[0].contents[0]} {self.delimiter} {pricev[0].contents[0]} {self.delimiter} {title[0].contents[0]} {self.delimiter} {htype[0].contents[0]} {self.delimiter} {descr[0].contents[0]} {self.delimiter} {item_url}')\n",
    "\n",
    "            url_to_search = self.next_page(soup)\n",
    "            k = i + k + 1\n",
    "\n",
    "def main():\n",
    "    print('Start')\n",
    "    app = WebScraping()\n",
    "    app.output_file = open(app.output_file_name,\"w+\")\n",
    "    app.read_website(app.main_url)\n",
    "    app.output_file.close()\n",
    "    print('End')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program to scrape www.inmuebles24.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing using only the start page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena.html\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena-pagina-2.html\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena-pagina-3.html\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena-pagina-4.html\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena-pagina-5.html\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena-pagina-6.html\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena-pagina-7.html\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena-pagina-8.html\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena-pagina-9.html\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena-pagina-10.html\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena-pagina-11.html\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena-pagina-12.html\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena-pagina-13.html\n",
      "Searching https://www.inmuebles24.com/inmuebles-en-jardin-balbuena-pagina-14.html\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import json\n",
    "\n",
    "class WebScraping(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        json_file_name = 'ws_parameters.json'\n",
    "        source_to_test = 'inmuebles24'\n",
    "\n",
    "        # open json file to get page ID and token\n",
    "        with open(json_file_name, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.main_url = data['app']['sources'][source_to_test]['main_url']\n",
    "        self.root_url = data['app']['sources'][source_to_test]['root_url']\n",
    "        self.output_file_name = data['app']['sources'][source_to_test]['output_file_name']\n",
    "        self.debug = data['app']['sources'][source_to_test]['debug']\n",
    "        self.delimiter = data['app']['sources'][source_to_test]['delimiter']\n",
    "        \n",
    "        if self.debug == 'Y':\n",
    "            print('In WebScraping')\n",
    "            print(self.main_url)\n",
    "            \n",
    "    def write_file(self, p_line):\n",
    "        self.output_file.write(p_line + \"\\n\")\n",
    "\n",
    "    def next_page(self, p_soup):\n",
    "        new_url = 'None'\n",
    "        pag_container = p_soup.find_all(\"li\", class_=\"pag-go-next\")\n",
    "        if self.debug == 'Y':\n",
    "            print('Type: ' + str(type(pag_container)) + ' with ' + str(len(pag_container)) + ' elements')\n",
    "        for item in pag_container:\n",
    "            new_url = self.root_url + item.findChild(\"a\")['href']\n",
    "        return new_url\n",
    "    \n",
    "    def get_url(self, p_data_links, p_id, p_initial_id):\n",
    "        item_url = p_data_links[p_id-p_initial_id]['href']\n",
    "        if self.debug == 'Y':\n",
    "            print(item_url)\n",
    "        return item_url\n",
    "\n",
    "    def read_website(self, p_url):\n",
    "        if self.debug == 'Y':\n",
    "            print('In read_website')\n",
    "        \n",
    "        url_to_search = p_url\n",
    "        k = 0 # To manage the counter accross pages\n",
    "        prices_list = list()\n",
    "        line = \"Id\" + self.delimiter + \"URL\" + self.delimiter + \"Descripci칩n\" + self.delimiter + \"Precio\" + self.delimiter + \\\n",
    "                \"Terreno\" + self.delimiter + \"Construcci칩n\" + self.delimiter + \"Rec치maras\" + self.delimiter + \\\n",
    "            \"Ba침os\" + self.delimiter + \"Estacionamiento\"\n",
    "        self.write_file(line)\n",
    "        \n",
    "        while url_to_search != 'None':\n",
    "            print('Searching ' + url_to_search)\n",
    "            page = requests.get(url_to_search)\n",
    "            soup = bs4.BeautifulSoup(page.content, 'html.parser')\n",
    "            data_block = soup.find_all(\"div\", class_=\"posting-info\")\n",
    "            data_links = soup.find_all(\"a\", class_=\"go-to-posting\")\n",
    "            data_prices = soup.find_all(\"span\", class_=\"first-price\")\n",
    "            \n",
    "            if self.debug == 'Y':\n",
    "                print('data_block type: ' + str(type(data_block)) + ' with ' + str(len(data_block)) + ' elements')\n",
    "                print('data_links type: ' + str(type(data_links)) + ' with ' + str(len(data_links)) + ' elements')\n",
    "                print('data_prices type: ' + str(type(data_prices)) + ' with ' + str(len(data_prices)) + ' elements')\n",
    "\n",
    "            for i, item in enumerate(data_prices):\n",
    "                item_price = item.text\n",
    "                prices_list.append(item_price)\n",
    "                \n",
    "            for i, item in enumerate(data_block):\n",
    "                item_anchor = item.find_all(\"a\", class_=\"go-to-posting\")\n",
    "                item_features_ul = item.find_all(\"ul\", class_=\"main-features go-to-posting\")\n",
    "                item_description_div = item.find_all(\"div\", class_=\"posting-description go-to-posting\")\n",
    "                item_url = self.get_url(data_links, i+k, k)\n",
    "                line = str(i+k+1) + self.delimiter + self.root_url + item_anchor[0][\"href\"] + self.delimiter + \\\n",
    "                        item_description_div[0].text.strip().replace(\"\\t\",\" \") + self.delimiter + \\\n",
    "                        prices_list[i] + self.delimiter + item_features_ul[0].text.strip().replace(\"\\n\",delimiter)\n",
    "                self.write_file(line)\n",
    "\n",
    "                if self.debug == 'Y':\n",
    "                    print(f'{str(i+k+1)} {self.delimiter} {self.root_url + item_anchor[0][\"href\"] } {self.delimiter} {item_description_div[0].text.strip()} {self.delimiter} {item_features_ul[0].text} {self.delimiter}')\n",
    "\n",
    "            url_to_search = self.next_page(soup)\n",
    "            k = i + k + 1\n",
    "\n",
    "def main():\n",
    "    print('Start')\n",
    "    app = WebScraping()\n",
    "    app.output_file = open(app.output_file_name,\"w+\")\n",
    "    app.read_website(app.main_url)\n",
    "    app.output_file.close()\n",
    "    print('End')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"test.txt\",\"w+\")\n",
    "soup = bs4.BeautifulSoup(page.content, 'html.parser')\n",
    "data_block = soup.find_all(\"div\", class_=\"posting-info\")\n",
    "print('Type: ' + str(type(data_block)) + ' with ' + str(len(data_block)) + ' elements')\n",
    "#print(soup.prettify())\n",
    "#print(str(data_block))\n",
    "f.write(str(data_block))\n",
    "#data_links = soup.find_all(\"a\", class_=\"item__info-link\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "delimiter = '\\t'\n",
    "print(\"Starting loop\")\n",
    "for item in data_block:\n",
    "#    print(str(i) + ' Type: ' + str(type(item)) + ' Name: ' + item.name)\n",
    "#    print(item['class'])\n",
    "    item_anchor = item.find_all(\"a\", class_=\"go-to-posting\")\n",
    "    item_url = root_url + item_anchor[0][\"href\"]\n",
    "    item_features_ul = item.find_all(\"ul\", class_=\"main-features go-to-posting\")\n",
    "    item_features = item_features_ul[0].text.replace(\"\\n\",delimiter)\n",
    "    item_description_div = item.find_all(\"div\", class_=\"posting-description go-to-posting\")\n",
    "    item_description = item_description_div[0].text.strip()\n",
    "    print('*******')\n",
    "    print(item_features.strip())\n",
    "    print('********************')\n",
    "    print(item_description)\n",
    "    print('*******')\n",
    "    print(item_url)\n",
    "#    print('Value: ' + str(item))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Del Carmen Coyoac치n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://listado.metroscubicos.com/santa-luc%C3%ADa-reacomodo#D[A:santa-luc%C3%ADa-reacomodo]\"\n",
    "page = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(page.content, 'html.parser')\n",
    "data_block = soup.find_all(\"div\", class_=\"item__info\")\n",
    "print('Type: ' + str(type(data_block)) + ' with ' + str(len(data_block)) + ' elements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdelimiter = '\\t'\n",
    "for i, item in enumerate(data_block):\n",
    "    prices = item.find_all(\"span\", class_=\"price__symbol\")\n",
    "    pricev = item.find_all(\"span\", class_=\"price__fraction\")\n",
    "    title = item.find_all(\"div\", class_=\"item__title\")\n",
    "    htype = item.find_all(\"p\", class_=\"item__info-title\")\n",
    "    descr = item.find_all(\"div\", class_=\"item__attrs\")\n",
    "    print(item)\n",
    "#    print(f'{str(i+1)} {self.delimiter} {prices[0].contents[0]} {self.delimiter} {pricev[0].contents[0]} {self.delimiter} {title[0].contents[0]} {self.delimiter} {htype[0].contents[0]} {self.delimiter} {descr[0].contents[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_links = soup.find_all(\"a\", class_=\"item__info-link\")\n",
    "print('Type: ' + str(type(data_links)) + ' with ' + str(len(data_links)) + ' elements')\n",
    "for i, item in enumerate(data_links):\n",
    "    #descr = item.find_all(\"div\", class_=\"item__attrs\")\n",
    "    print(item['href'])\n",
    "#    print(f'{str(i+1)} {self.delimiter} {prices[0].contents[0]} {self.delimiter} {pricev[0].contents[0]} {self.delimiter} {title[0].contents[0]} {self.delimiter} {htype[0].contents[0]} {self.delimiter} {descr[0].contents[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pag_container = soup.find_all(\"a\", class_=\"andes-pagination__link prefetch\")\n",
    "print('Type: ' + str(type(pag_container)) + ' with ' + str(len(pag_container)) + ' elements')\n",
    "for item in pag_container:\n",
    "    nurl = item.get('href')\n",
    "    print(nurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4\n",
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
